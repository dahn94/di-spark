{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30a0ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import StructField, LongType\n",
    "from pyspark.sql import types\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import pandas as pd\n",
    "from delta import *\n",
    "from delta.tables import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c8e787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee996e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+----------+----+---------+--------------+\n",
      "|dataInicial| dataFinal|percentual|     fator|taxa|valorBase|valorCalculado|\n",
      "+-----------+----------+----------+----------+----+---------+--------------+\n",
      "| 04/07/1994|05/07/1994|     100.0|   1.00398| 0.4|   1000.0|       1003.98|\n",
      "| 05/07/1994|06/07/1994|     100.0|1.00399667| 0.4|   1000.0|       1003.99|\n",
      "| 06/07/1994|07/07/1994|     100.0|     1.004| 0.4|   1000.0|        1004.0|\n",
      "| 07/07/1994|08/07/1994|     100.0|   1.00397| 0.4|   1000.0|       1003.97|\n",
      "| 08/07/1994|09/07/1994|     100.0|   1.00389|0.39|   1000.0|       1003.89|\n",
      "| 09/07/1994|10/07/1994|     100.0|       1.0| 0.0|   1000.0|        1000.0|\n",
      "| 10/07/1994|11/07/1994|     100.0|       1.0| 0.0|   1000.0|        1000.0|\n",
      "| 11/07/1994|12/07/1994|     100.0|1.00375667|0.38|   1000.0|       1003.75|\n",
      "| 12/07/1994|13/07/1994|     100.0|    1.0038|0.38|   1000.0|        1003.8|\n",
      "| 13/07/1994|14/07/1994|     100.0|1.00380333|0.38|   1000.0|        1003.8|\n",
      "| 14/07/1994|15/07/1994|     100.0|1.00374667|0.37|   1000.0|       1003.74|\n",
      "| 15/07/1994|16/07/1994|     100.0|1.00342667|0.34|   1000.0|       1003.42|\n",
      "| 16/07/1994|17/07/1994|     100.0|       1.0| 0.0|   1000.0|        1000.0|\n",
      "| 17/07/1994|18/07/1994|     100.0|       1.0| 0.0|   1000.0|        1000.0|\n",
      "| 18/07/1994|19/07/1994|     100.0|1.00332333|0.33|   1000.0|       1003.32|\n",
      "| 19/07/1994|20/07/1994|     100.0|1.00337667|0.34|   1000.0|       1003.37|\n",
      "| 20/07/1994|21/07/1994|     100.0|1.00336333|0.34|   1000.0|       1003.36|\n",
      "| 21/07/1994|22/07/1994|     100.0|1.00276333|0.28|   1000.0|       1002.76|\n",
      "| 22/07/1994|23/07/1994|     100.0|   1.00238|0.24|   1000.0|       1002.38|\n",
      "| 23/07/1994|24/07/1994|     100.0|       1.0| 0.0|   1000.0|        1000.0|\n",
      "+-----------+----------+----------+----------+----+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"data_entrada.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e606b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      data|     fator|\n",
      "+----------+----------+\n",
      "|05/07/1994|   1.00398|\n",
      "|06/07/1994|1.00399667|\n",
      "|07/07/1994|     1.004|\n",
      "|08/07/1994|   1.00397|\n",
      "|09/07/1994|   1.00389|\n",
      "|12/07/1994|1.00375667|\n",
      "|13/07/1994|    1.0038|\n",
      "|14/07/1994|1.00380333|\n",
      "|15/07/1994|1.00374667|\n",
      "|16/07/1994|1.00342667|\n",
      "|19/07/1994|1.00332333|\n",
      "|20/07/1994|1.00337667|\n",
      "|21/07/1994|1.00336333|\n",
      "|22/07/1994|1.00276333|\n",
      "|23/07/1994|   1.00238|\n",
      "|26/07/1994|1.00244667|\n",
      "|27/07/1994|   1.00234|\n",
      "|28/07/1994|1.00245333|\n",
      "|29/07/1994|1.00209667|\n",
      "|30/07/1994|1.00183333|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('dataInicial', 'percentual', 'taxa', 'valorBase', 'valorCalculado')\n",
    "df = df.withColumnRenamed(\"dataFinal\", \"data\")\n",
    "df = df.where(\"fator>1.00000000\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e46b471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---+\n",
      "|      data|     fator| id|\n",
      "+----------+----------+---+\n",
      "|05/07/1994|   1.00398|  0|\n",
      "|06/07/1994|1.00399667|  1|\n",
      "|07/07/1994|     1.004|  2|\n",
      "|08/07/1994|   1.00397|  3|\n",
      "|09/07/1994|   1.00389|  4|\n",
      "|12/07/1994|1.00375667|  5|\n",
      "|13/07/1994|    1.0038|  6|\n",
      "|14/07/1994|1.00380333|  7|\n",
      "|15/07/1994|1.00374667|  8|\n",
      "|16/07/1994|1.00342667|  9|\n",
      "|19/07/1994|1.00332333| 10|\n",
      "|20/07/1994|1.00337667| 11|\n",
      "|21/07/1994|1.00336333| 12|\n",
      "|22/07/1994|1.00276333| 13|\n",
      "|23/07/1994|   1.00238| 14|\n",
      "|26/07/1994|1.00244667| 15|\n",
      "|27/07/1994|   1.00234| 16|\n",
      "|28/07/1994|1.00245333| 17|\n",
      "|29/07/1994|1.00209667| 18|\n",
      "|30/07/1994|1.00183333| 19|\n",
      "+----------+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = df.schema.add(StructField(\"id\", LongType()))\n",
    "rdd = df.rdd.zipWithIndex()\n",
    "\n",
    "\n",
    "def flat(l):\n",
    "    for k in l:\n",
    "        if not isinstance(k, (list, tuple)):\n",
    "            yield k\n",
    "        else:\n",
    "            yield from flat(k)\n",
    "\n",
    "\n",
    "rdd = rdd.map(lambda x: list(flat(x)))\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a5574f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/26 23:59:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------+--------------+\n",
      "|      data|     fator|fator_acumulado|taxa_acumulada|\n",
      "+----------+----------+---------------+--------------+\n",
      "|05/07/1994|   1.00398|        1.00398|    0.39800406|\n",
      "|06/07/1994|1.00399667|      1.0079926|     0.7992625|\n",
      "|07/07/1994|     1.004|      1.0120245|     1.2024522|\n",
      "|08/07/1994|   1.00397|      1.0160422|     1.6042233|\n",
      "|09/07/1994|   1.00389|      1.0199947|     1.9994736|\n",
      "|12/07/1994|1.00375667|      1.0238265|      2.382648|\n",
      "|13/07/1994|    1.0038|       1.027717|     2.7716994|\n",
      "|14/07/1994|1.00380333|      1.0316257|     3.1625748|\n",
      "|15/07/1994|1.00374667|      1.0354909|      3.549087|\n",
      "|16/07/1994|1.00342667|      1.0390393|     3.9039254|\n",
      "|19/07/1994|1.00332333|      1.0424923|      4.249227|\n",
      "|20/07/1994|1.00337667|      1.0460124|       4.60124|\n",
      "|21/07/1994|1.00336333|      1.0495305|     4.9530506|\n",
      "|22/07/1994|1.00276333|      1.0524307|      5.243075|\n",
      "|23/07/1994|   1.00238|      1.0549355|     5.4935455|\n",
      "|26/07/1994|1.00244667|      1.0575166|     5.7516575|\n",
      "|27/07/1994|   1.00234|      1.0599911|      5.999112|\n",
      "|28/07/1994|1.00245333|      1.0625917|      6.259167|\n",
      "|29/07/1994|1.00209667|      1.0648196|     6.4819574|\n",
      "|30/07/1994|1.00183333|      1.0667717|     6.6771746|\n",
      "+----------+----------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window = Window.orderBy('id')\n",
    "mul_udf = F.udf(lambda x: reduce(mul, x), types.DoubleType())\n",
    "df = df.withColumn('fator_acumulado', mul_udf(F.collect_list(F.col('fator')).over(window)))\n",
    "\n",
    "# Converting String based numbers into float.\n",
    "df = df.withColumn('fator_acumulado', df.fator_acumulado.cast(\"float\"))\n",
    "\n",
    "\n",
    "# Function defined by user, to calculate distance between two points on the globe.\n",
    "def get_taxaAcumulada(fator_acumulado):\n",
    "    taxa_acumulada = (fator_acumulado - 1) * 100\n",
    "\n",
    "    return taxa_acumulada\n",
    "\n",
    "\n",
    "udf_func = udf(get_taxaAcumulada, FloatType())\n",
    "df = df.withColumn(\"taxa_acumulada\", udf_func(df.fator_acumulado))\n",
    "df = df.drop('id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b5ef68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/26 23:59:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"delta\").save(\"saida-delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a5f2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_df = spark.read.format(\"delta\").load(\"saida-delta-table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
